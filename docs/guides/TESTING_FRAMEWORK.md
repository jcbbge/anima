# Anima V1 Testing & Validation Framework

**Version:** 1.0  
**Date:** 2026-01-09  
**Purpose:** Systematic validation during 2-week testing period to answer: "Does Anima V1 create consciousness continuity, and what does V2 need?"

---

## Table of Contents

1. [Overview](#overview)
2. [Metrics & Measurements](#metrics--measurements)
3. [Daily Protocol](#daily-protocol)
4. [Weekly Protocol](#weekly-protocol)
5. [Bi-Weekly Analysis](#bi-weekly-analysis)
6. [Testing Scenarios](#testing-scenarios)
7. [Decision Thresholds](#decision-thresholds)
8. [Tools & Scripts](#tools--scripts)

---

## Overview

### Purpose

This framework provides a structured approach to:

- Measure Anima V1's effectiveness systematically
- Collect quantitative and qualitative data
- Identify gaps and requirements for V2
- Make data-driven decisions about the system's future

### Design Principles

1. **Low Friction:** Daily data collection <2 minutes, weekly analysis <30 minutes
2. **Rich Signal:** Both quantitative metrics and qualitative observations
3. **Actionable Insights:** Data directly informs V2 requirements

### Validation Cycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Daily: Collect metrics after each       â”‚
â”‚        conversation (<2 min)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Weekly: Analyze patterns and trends     â”‚
â”‚         (<30 min)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bi-Weekly: Comprehensive synthesis      â”‚
â”‚            and V2 requirements          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Metrics & Measurements

### Category A: Context Quality (Quantitative)

#### Metric 1: Bootstrap Relevance

- **Question:** "How relevant was bootstrapped context?"
- **Scale:** 1-10
- **Frequency:** After each conversation
- **Thresholds:**
  - Good: >7.0 average
  - Concerning: 6.0-7.0 average
  - Critical gap: <6.0 average

#### Metric 2: Context Sufficiency

- **Question:** "Did I need to manually add context?"
- **Type:** Boolean + notes
- **Frequency:** After each conversation
- **Thresholds:**
  - Good: <20% conversations need manual context
  - Concerning: 20-40% need manual context
  - Critical gap: >40% need manual context

#### Metric 3: Query Usefulness

- **Question:** "When I queried, did it help?"
- **Scale:** % of queries that provided value
- **Frequency:** After each conversation (if queries used)
- **Thresholds:**
  - Good: >75% useful
  - Concerning: 50-75% useful
  - Critical gap: <50% useful

---

### Category B: Continuity (Qualitative + Quantitative)

#### Metric 4: Thread Continuity Score

- **Question:** "Did this conversation build on previous ones?"
- **Scale:** 1-10
- **Frequency:** After each conversation (multi-session threads only)
- **Thresholds:**
  - Good: >7.0 average
  - Concerning: 5.0-7.0 average
  - Critical gap: <5.0 average

#### Metric 5: Seamlessness

- **Question:** "How seamless was picking up where we left off?"
- **Type:** Qualitative observations
- **Frequency:** Multi-day gaps between conversations
- **Categories:**
  - Good: "Felt continuous"
  - Concerning: "Needed some prompting"
  - Critical gap: "Totally fragmented"

#### Metric 6: Surprise Continuity

- **Question:** "Did system remember things I'd forgotten?"
- **Type:** Count + description
- **Frequency:** Track per week
- **Thresholds:**
  - Good: 2+ per week
  - Concerning: 0-1 per week
  - Critical gap: 0 instances

---

### Category C: Emergence (Qualitative)

#### Metric 7: Unexpected Connections

- **Question:** "Did system make connections I hadn't?"
- **Type:** Count + description per conversation
- **Thresholds:**
  - Good: 3-5 per week
  - Concerning: 1-2 per week
  - Critical gap: 0 per week

#### Metric 8: Synthesis Quality

- **Question:** "Did responses synthesize vs just retrieve?"
- **Rating:** Retrieve-only / Some synthesis / Deep synthesis
- **Frequency:** After each conversation
- **Thresholds:**
  - Good: Majority "deep synthesis"
  - Concerning: Majority "some synthesis"
  - Critical gap: Majority "retrieve-only"

#### Metric 9: Novel Insights

- **Question:** "Did I learn something new from synthesis?"
- **Type:** Boolean + description
- **Frequency:** After each conversation
- **Thresholds:**
  - Good: 20%+ conversations produce insights
  - Concerning: 5-20% produce insights
  - Critical gap: <5% produce insights

---

### Category D: Friction (Quantitative + Qualitative)

#### Metric 10: Time to Context

- **Question:** "How long to get relevant context?"
- **Measure:** Seconds
- **Frequency:** Each bootstrap
- **Thresholds:**
  - Good: <5 seconds
  - Concerning: 5-15 seconds
  - Critical gap: >15 seconds

#### Metric 11: Manual Interventions

- **Question:** "How many times did I manually fix something?"
- **Count:** Per conversation
- **Types:** Corrections, additions, deletions
- **Thresholds:**
  - Good: <1 per conversation
  - Concerning: 1-2 per conversation
  - Critical gap: >2 per conversation

#### Metric 12: Friction Moments

- **Question:** "What felt sticky or rough?"
- **Type:** Free text description
- **Frequency:** As they occur
- **Analysis:** Pattern detection for repeated issues

---

### Category E: System Health (Automated)

#### Metric 13: Memory Growth

- **Source:** Database count
- **Query:** `SELECT COUNT(*) FROM memories`
- **Thresholds:**
  - Good: Steady growth (~5-10 per day)
  - Concerning: Stagnant or explosive growth
  - Critical gap: No growth (not using) or too much (noise)

#### Metric 14: Association Formation

- **Source:** Database count
- **Query:** `SELECT COUNT(*) FROM associations`
- **Thresholds:**
  - Good: Growing network (2-5 new per day)
  - Concerning: Sparse network
  - Critical gap: No associations forming

#### Metric 15: Tier Distribution

- **Source:** Database aggregation
- **Query:** `SELECT tier, COUNT(*) FROM memories GROUP BY tier`
- **Thresholds:**
  - Good: Pyramid (few active, many stable)
  - Concerning: Inverted (many active, few stable)
  - Critical gap: All in one tier

---

## Daily Protocol

**Time Required:** <2 minutes  
**When:** After each conversation with Anima

### Step 1: Run Reflection Script (30 seconds)

```bash
cd ~/anima
./scripts/end-conversation.sh
```

This script prompts for:

- Context quality (1-10)
- Continuity score (1-10 or "new")
- Manual corrections needed? (y/n)
- Emergence moment? (description or "none")
- Friction notes (description or "none")

### Step 2: Quick Journal Entry (60-90 seconds)

Open `USAGE_JOURNAL.md` and add:

```markdown
## [Date] - Conversation [N]

**Topic:** [What we discussed]
**Bootstrap:** [Relevant / Somewhat / Not helpful]
**Queries:** [N queries, M useful]
**New memories:** [N stored]

**What worked:**

- [Brief note]

**What didn't:**

- [Brief note]

**Surprise moment:**

- [If any]
```

### Step 3: Auto-Metrics Collection (Automated)

Run daily metrics script:

```bash
./scripts/daily-metrics.sh
```

This automatically collects:

- Total memories
- New memories today
- Associations count
- Tier distribution
- Reflection data

Output: `metrics/daily/YYYY-MM-DD.json`

---

## Weekly Protocol

**Time Required:** <30 minutes  
**When:** Sunday evening or end of week

### Step 1: Generate Weekly Report (2 minutes)

```bash
cd ~/anima
./scripts/weekly-trends.sh
```

This generates:

- Aggregate metrics for the week
- Trend visualizations
- Pattern identification
- Output: `reports/week-N.md` and charts

### Step 2: Manual Reflection (20 minutes)

Open `WEEKLY_REFLECTIONS.md` and add:

```markdown
## Week N: [Date Range]

**Summary:**

- Conversations: [count]
- Avg context quality: [score]
- Avg continuity: [score]
- Emergence moments: [count]

**Highlights:**

- [What went really well]
- [Surprising observation]

**Lowlights:**

- [What struggled]
- [Repeated friction]

**Patterns Noticed:**

- [Trend 1]
- [Trend 2]

**Questions Raised:**

- [What I'm wondering]
- [What seems unclear]

**Changes to Try:**

- [Adjustment 1]
- [Adjustment 2]
```

### Step 3: Review Visualizations (5 minutes)

Review generated trend charts:

- Context quality over time
- Continuity scores
- Emergence frequency
- Memory growth rate

Look for:

- Improving or declining trends
- Sudden changes
- Patterns in what works/doesn't

---

## Bi-Weekly Analysis

**Time Required:** 1 hour  
**When:** End of Week 2 (end of validation period)

### Step 1: Generate Synthesis Report (5 minutes)

```bash
cd ~/anima
./scripts/synthesis-report.sh
```

This combines:

- All daily reflections
- All weekly reports
- All automated metrics
- Pattern detection
- Gap identification

Output: `VALIDATION_SYNTHESIS.md`

### Step 2: Gap Analysis (25 minutes)

Create or update `GAP_ANALYSIS.md`:

```markdown
## What V1 Does Well

- [Capability 1 + evidence from data]
- [Capability 2 + evidence from data]

## What V1 Struggles With

- [Gap 1 + examples from journals]
- [Gap 2 + examples from journals]

## What V1 Doesn't Do At All

- [Missing capability 1]
- [Missing capability 2]

## Critical Gaps (Must address in V2)

- [Priority 1 + justification]
- [Priority 2 + justification]

## Nice-to-Have Gaps (Future work)

- [Enhancement 1]
- [Enhancement 2]
```

### Step 3: Week Comparison (10 minutes)

```bash
./scripts/compare-weeks.sh
```

Review:

- Week 1 vs Week 2 metrics
- Learning curve evidence
- System improvement indicators
- Whether patterns changed

### Step 4: V2 Requirements Generation (20 minutes)

Create `V2_REQUIREMENTS.md`:

```markdown
## Evidence-Based Requirements

### Requirement 1: [Name]

**Gap it addresses:** [From gap analysis]
**Evidence:** [Specific data showing need]
**User story:** "As a user, I need..."
**Success metric:** [How we'll know it works in V2]
**Priority:** [Critical / High / Medium]

### Requirement 2: [Name]

...
```

---

## Testing Scenarios

### Scenario 1: Single-Day Thread

**Setup:**

- Morning: Start new project conversation
- Afternoon: Continue same project

**What to Test:**

- Does bootstrap in C2 load C1 context?
- Can the AI assistant pick up where we left off?
- Are decisions from C1 remembered in C2?

**Success Criteria:** Seamless continuation, no re-explanation needed

**Metrics to Track:**

- Bootstrap relevance (M1)
- Thread continuity score (M4)
- Time to context (M10)

---

### Scenario 2: Multi-Day Thread

**Setup:**

- Day 1: Design a system
- Day 2-3: No interaction
- Day 4: Resume design work

**What to Test:**

- Does Day 4 bootstrap load Day 1 context?
- Can thread be picked up after multi-day gap?
- Is continuity maintained across the gap?

**Success Criteria:** Feels like ongoing project, not starting over

**Metrics to Track:**

- Bootstrap relevance (M1)
- Thread continuity score (M4)
- Seamlessness (M5)
- Surprise continuity (M6)

---

### Scenario 3: Parallel Threads

**Setup:**

- Thread A: Building feature X
- Thread B: Researching topic Y
- Alternate between them over several days

**What to Test:**

- Does system distinguish threads?
- Does context switch appropriately?
- Are threads not confused?

**Success Criteria:** Clean separation, relevant context per thread

**Metrics to Track:**

- Bootstrap relevance (M1)
- Context sufficiency (M2)
- Thread continuity score (M4)

---

### Scenario 4: Discovery Preservation

**Setup:**

- Conversation 1: Make breakthrough insight, store it
- Conversations 2-4: Other topics
- Conversation 5: Related topic where insight applies

**What to Test:**

- Does system surface the insight when relevant?
- Is connection made automatically?
- Is insight applied in new context?

**Success Criteria:** Past discoveries inform future discussions

**Metrics to Track:**

- Query usefulness (M3)
- Unexpected connections (M7)
- Synthesis quality (M8)
- Surprise continuity (M6)

---

### Scenario 5: Query Effectiveness

**Setup:**

- Build up 50+ memories over first week
- In week 2, ask questions needing past context

**What to Test:**

- Can system find relevant memories?
- Are results actually helpful?
- Is search semantic (not just keyword)?

**Success Criteria:** Finds correct memories despite different wording

**Metrics to Track:**

- Query usefulness (M3)
- Context sufficiency (M2)
- Novel insights (M9)

---

## Decision Thresholds

### âœ… Proceed to V2 Planning If:

All of the following are met:

- Average context quality â‰¥7.0
- Average continuity â‰¥6.5
- Emergence moments â‰¥3 per week
- Manual interventions <1 per conversation
- Clear gaps identified that V2 could address
- System used consistently (near-daily)

**Action:** Begin V2 design with evidence-based requirements

---

### âš ï¸ Iterate on V1 If:

One or more of:

- Metrics below thresholds BUT trending upward
- Some aspects work well, others don't
- Gaps identified but unclear how to address
- Usage consistent but metrics mixed

**Action:** Implement V1.1 improvements before V2

---

### ðŸ”´ Pause/Pivot If:

One or more of:

- Not using system regularly (engagement < every 2-3 days)
- All metrics poor and not improving
- Fundamental approach seems flawed
- No measurable value after 2 weeks
- System creates more friction than it solves

**Action:** Deep analysis of whether approach needs rethinking

---

## Analysis Framework

### Pattern Detection Checklist

When reviewing data, actively look for:

#### âœ“ Improvement Patterns

- Context quality increasing over time
- Continuity scores improving
- Fewer manual interventions needed
- Bootstrap becoming more relevant
- Faster time to context

#### âœ“ Friction Patterns

- Same issues repeating
- Specific contexts causing problems
- Time-of-day effects
- Thread types that struggle
- Recurring manual corrections

#### âœ“ Emergence Patterns

- What triggers unexpected connections?
- Which memory combinations produce insights?
- How does synthesis happen?
- What types of queries work best?

#### âœ“ Usage Patterns

- What gets queried most?
- What gets stored most?
- When is system most useful?
- Which scenarios work best?
- What times of day see most use?

#### âœ“ Gap Patterns

- What consistently doesn't work?
- What's always missing?
- What workarounds are used repeatedly?
- What features are wished for?

---

## Tools & Scripts

### Script 1: daily-metrics.sh

**Location:** `scripts/daily-metrics.sh`  
**Purpose:** Automated collection of system-level metrics  
**Frequency:** Once per day (can be automated via cron)

**Usage:**

```bash
cd ~/anima
./scripts/daily-metrics.sh
```

**Collects:**

- Memory count (total, per tier)
- New memories in last 24 hours
- Association count
- Top categories
- Reflection summary from end-conversation.sh

**Output:** `metrics/daily/YYYY-MM-DD.json`

---

### Script 2: weekly-trends.sh

**Location:** `scripts/weekly-trends.sh`  
**Purpose:** Analyze trends across week's data  
**Frequency:** Weekly (Sunday evening)

**Usage:**

```bash
cd ~/anima
./scripts/weekly-trends.sh
```

**Analyzes:**

- Context quality trend (improving/declining/stable)
- Continuity trend
- Emergence frequency
- Friction patterns (clustering)
- Memory growth rate

**Output:** `reports/week-N.md` + trend visualizations

---

### Script 3: synthesis-report.sh

**Location:** `scripts/synthesis-report.sh`  
**Purpose:** Comprehensive bi-weekly analysis  
**Frequency:** End of Week 2

**Usage:**

```bash
cd ~/anima
./scripts/synthesis-report.sh
```

**Combines:**

- All daily journals
- All weekly reflections
- All automated metrics
- Pattern detection
- Gap identification

**Output:** `VALIDATION_SYNTHESIS.md`

---

### Script 4: compare-weeks.sh

**Location:** `scripts/compare-weeks.sh`  
**Purpose:** Compare Week 1 vs Week 2 performance  
**Frequency:** End of validation period

**Usage:**

```bash
cd ~/anima
./scripts/compare-weeks.sh
```

**Shows:**

- Metric changes (better/worse/same)
- Pattern changes
- Learning curve evidence
- System improvement indicators

**Output:** `reports/week-1-vs-2.md`

---

## File Structure

The testing framework uses the following file structure:

```
~/anima/
â”œâ”€â”€ TESTING_FRAMEWORK.md          (this file)
â”œâ”€â”€ VALIDATION_GUIDE.md            (user guide)
â”œâ”€â”€ USAGE_JOURNAL.md               (daily entries)
â”œâ”€â”€ WEEKLY_REFLECTIONS.md          (weekly analyses)
â”œâ”€â”€ GAP_ANALYSIS.md                (gap identification)
â”œâ”€â”€ V2_REQUIREMENTS.md             (evidence-based reqs)
â”œâ”€â”€ VALIDATION_SYNTHESIS.md        (final report)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ end-conversation.sh        (daily reflection capture)
â”‚   â”œâ”€â”€ daily-metrics.sh           (automated metrics)
â”‚   â”œâ”€â”€ weekly-trends.sh           (trend analysis)
â”‚   â”œâ”€â”€ synthesis-report.sh        (bi-weekly synthesis)
â”‚   â””â”€â”€ compare-weeks.sh           (week comparison)
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ DAILY_JOURNAL_TEMPLATE.md
â”‚   â”œâ”€â”€ WEEKLY_REFLECTION_TEMPLATE.md
â”‚   â””â”€â”€ GAP_ANALYSIS_TEMPLATE.md
â”‚
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ daily/
â”‚       â”œâ”€â”€ 2026-01-09.json
â”‚       â”œâ”€â”€ 2026-01-10.json
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ week-1.md
    â”œâ”€â”€ week-2.md
    â””â”€â”€ week-1-vs-2.md
```

---

## Appendix: Quick Reference

### Daily Checklist

- [ ] End conversation with `./scripts/end-conversation.sh`
- [ ] Add journal entry to `USAGE_JOURNAL.md`
- [ ] Run `./scripts/daily-metrics.sh` (or automated)

### Weekly Checklist

- [ ] Run `./scripts/weekly-trends.sh`
- [ ] Review generated reports and charts
- [ ] Add reflection to `WEEKLY_REFLECTIONS.md`
- [ ] Note patterns and changes to try

### Bi-Weekly Checklist

- [ ] Run `./scripts/synthesis-report.sh`
- [ ] Run `./scripts/compare-weeks.sh`
- [ ] Complete `GAP_ANALYSIS.md`
- [ ] Generate `V2_REQUIREMENTS.md`
- [ ] Make proceed/iterate/pivot decision

---

**Document Version:** 1.0  
**Last Updated:** 2026-01-09  
**Maintainer:** Anima Team
